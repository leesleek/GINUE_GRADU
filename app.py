import streamlit as st
import os
import glob
import cohere
from openai import OpenAI
import chromadb
from pypdf import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import re

# --- [1. ê¸°ë³¸ ì„¤ì • ë° ì œëª©] ---
st.set_page_config(page_title="ê²½ì¸êµìœ¡ëŒ€í•™êµ ëŒ€í•™ì› ê·œì • ì±—ë´‡", page_icon="ğŸ“")

# ì œëª© ìŠ¤íƒ€ì¼
st.markdown(
    """
    <h1 style='text-align: center; font-size: 36px; margin-bottom: 30px;'>
        ğŸ“ ê²½ì¸êµìœ¡ëŒ€í•™êµ ëŒ€í•™ì› ê·œì • ì•ˆë‚´ AI
    </h1>
    """, 
    unsafe_allow_html=True
)

# --- [ê¸°ëŠ¥ ì¶”ê°€: ëŒ€í™” ì´ˆê¸°í™” í•¨ìˆ˜] ---
def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ëŒ€í•™ì› ê·œì •ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}]

# --- [ì‚¬ì´ë“œë°” ì„¤ì •] ---
with st.sidebar:
    # 1. ìƒˆë¡œìš´ ì±„íŒ… ë²„íŠ¼
    if st.button("ğŸ”„ ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘", type="primary", use_container_width=True):
        clear_chat_history()
        st.rerun()
        
    st.markdown("---")

    # 2. ì •ë³´ ë° ë¼ì´ì„¼ìŠ¤
    st.header("ì •ë³´")
    st.info("ì´ ì±—ë´‡ì€ ê²½ì¸êµìœ¡ëŒ€í•™êµ ëŒ€í•™ì› ê·œì • PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.")
    
    st.markdown("<br>" * 8, unsafe_allow_html=True)
    st.markdown("---")
    
    # ë¼ì´ì„¼ìŠ¤ í‘œê¸°
    st.markdown(
        """
        <div style='text-align: center; color: grey; font-size: 12px;'>
            Developed by <br>
            <b>Prof. LCH</b> <br>
            (<a href='mailto:leesleek@ginue.ac.kr' style='text-decoration: none; color: grey;'>leesleek@ginue.ac.kr</a>)
        </div>
        """, 
        unsafe_allow_html=True
    )

# --- [2. API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”] ---
@st.cache_resource
def init_clients():
    try:
        co = cohere.Client(st.secrets["COHERE_API_KEY"])
        openai_client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
        chroma_client = chromadb.Client()
        return co, openai_client, chroma_client
    except Exception as e:
        st.error("API í‚¤ ì„¤ì • ì˜¤ë¥˜: .streamlit/secrets.toml íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None, None, None

co, openai_client, chroma_client = init_clients()

# --- [3. ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶• í•¨ìˆ˜] ---
@st.cache_resource
def load_and_index_pdfs():
    collection_name = "pdf_knowledge_base"
    
    try:
        chroma_client.delete_collection(collection_name)
    except:
        pass
        
    collection = chroma_client.create_collection(name=collection_name)
    
    # [ìˆ˜ì •ë¨] í´ë” ê²½ë¡œë¥¼ 'gradu_data'ë¡œ ë³€ê²½
    pdf_files = glob.glob("gradu_data/*.pdf")
    if not pdf_files:
        st.warning("âš ï¸ 'gradu_data' í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    status_text = st.empty()
    status_text.info("ğŸ“š ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.")

    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", " ", ""],
        chunk_size=1024,
        chunk_overlap=100,
        length_function=len,
    )

    all_chunks = []
    all_metadatas = []

    for file_path in pdf_files:
        try:
            reader = PdfReader(file_path)
            full_text = ""
            for page in reader.pages:
                text = page.extract_text()
                if text: full_text += text + "\n"
            
            full_text = re.sub(r'\n{3,}', '\n\n', full_text)
            
            chunks = text_splitter.split_text(full_text)
            for i, chunk in enumerate(chunks):
                all_chunks.append(chunk)
                all_metadatas.append({"source": os.path.basename(file_path)})
        except Exception as e:
            st.warning(f"{file_path} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    batch_size = 100
    for i in range(0, len(all_chunks), batch_size):
        batch_texts = all_chunks[i:i+batch_size]
        batch_metas = all_metadatas[i:i+batch_size]
        batch_ids = [str(hash(t)) for t in batch_texts]
        
        response = openai_client.embeddings.create(
            input=batch_texts,
            model="text-embedding-3-small"
        )
        embeddings = [data.embedding for data in response.data]
        
        collection.add(
            documents=batch_texts,
            embeddings=embeddings,
            metadatas=batch_metas,
            ids=batch_ids
        )
    
    status_text.success(f"âœ… ì´ {len(pdf_files)}ê°œì˜ ê·œì • ë¬¸ì„œ í•™ìŠµ ì™„ë£Œ!")
    return collection

if openai_client:
    collection = load_and_index_pdfs()

# --- [4. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤] ---

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ëŒ€í•™ì› ê·œì •ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        if not collection:
            st.error("ì§€ì‹ë² ì´ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            st.stop()

        # 1. ê²€ìƒ‰
        query_embed = openai_client.embeddings.create(
            input=[prompt],
            model="text-embedding-3-small"
        ).data[0].embedding
        
        results = collection.query(query_embeddings=[query_embed], n_results=30)
        retrieved_docs = results['documents'][0]
        retrieved_metas = results['metadatas'][0]

        if not retrieved_docs:
            full_response = "ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            message_placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            st.stop()

        # 2. Rerank
        rerank_results = co.rerank(
            query=prompt,
            documents=retrieved_docs,
            model="rerank-multilingual-v3.0",
            top_n=5
        )
        
        final_docs = []
        sources = set()
        for hit in rerank_results.results:
            final_docs.append(retrieved_docs[hit.index])
            sources.add(retrieved_metas[hit.index]['source'])

        context = "\n\n".join(final_docs)
        source_text = ", ".join(sources)

        # 3. ë‹µë³€ ìƒì„±
        system_prompt = f"""
        ë‹¹ì‹ ì€ ê²½ì¸êµìœ¡ëŒ€í•™êµ ëŒ€í•™ì› ê·œì • ì•ˆë‚´ AIì…ë‹ˆë‹¤.
        ì•„ë˜ì˜ [ì»¨í…ìŠ¤íŠ¸]ì™€ [ì°¸ê³  ë¬¸ì„œ ëª©ë¡]ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

        ë‹µë³€ ì‘ì„± ì§€ì¹¨:
        1. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì€ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
        2. ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ë˜, í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
        3. ë‹µë³€ì€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”.
        4. ì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

        ë‹µë³€ í˜•ì‹:
        - ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ëŠ” ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
        - í•„ìš”ì‹œ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ê±°ë‚˜ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”.
        - ì¶œì²˜ê°€ ëª…í™•í•œ ê²½ìš° í•´ë‹¹ ë¬¸ì„œë‚˜ ì„¹ì…˜ì„ ì–¸ê¸‰í•˜ì„¸ìš”.

        [ì»¨í…ìŠ¤íŠ¸]
        {context}
        
        [ì°¸ê³  ë¬¸ì„œ ëª©ë¡]
        {source_text}
        """

        full_response = ""
        stream = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True
        )
        
        for chunk in stream:
            if chunk.choices[0].delta.content:
                full_response += chunk.choices[0].delta.content
                message_placeholder.markdown(full_response + "â–Œ")
        
        message_placeholder.markdown(full_response)
        
        if sources:
            st.caption(f"ğŸ“š ì°¸ê³  ë¬¸ì„œ: {source_text}")
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})